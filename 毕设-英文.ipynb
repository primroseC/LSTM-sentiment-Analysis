{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test, Y_test = read_csv('data/data-English/train.csv')\n",
    "X_test, Y_test = read_csv('data/data-English/test.csv')\n",
    "\n",
    "X_train, Y_train = read_csv('data/data-English/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "def category(label):\n",
    "    a=['NEGATIVE','POSITIVE','SUGGESTION']\n",
    "    return a[label-1]\n",
    "print(category(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This course is more of an overview of what is to come NEGATIVE\n",
      "['This', 'course', 'is', 'more', 'of', 'an', 'overview', 'of', 'what', 'is', 'to', 'come']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for i in range (1,324):\n",
    "    A=re.split(r\". ;| |!|,|\\?\",X_train[i])\n",
    "    if '' in A:\n",
    "        print(i)\n",
    "        print(A)\n",
    "    i+=1\n",
    "'''\n",
    "index = 5\n",
    "#print(X_train[index], label_to_emoji(Y_train[index]))\n",
    "print(X_train[index], category(Y_train[index]))\n",
    "symbol=\"!,\"\n",
    "s=re.split(r\". ;| |!|,|\\?\",X_train[index])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 3)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 is converted into one hot [0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "index =109\n",
    "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "#读一个一行 一个词+50维\n",
    "#print(word_to_vec_map['it']) --这个是字典，对应的词和它的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of specialization in the vocabulary is 339014\n",
      "the 289846th word in the vocabulary is potatos\n",
      "[-0.53862   0.46822  -0.91289  -0.37902  -0.34028   0.33678   0.42091\n",
      " -0.6817    0.59982  -0.048546  0.74597  -0.25944   0.50634  -0.059825\n",
      " -0.85124   0.21286   0.17618   0.50062   0.62398   0.14575   0.31584\n",
      " -0.33425  -0.19406   0.18952  -0.21367   0.33764  -0.20979  -0.51248\n",
      "  0.033012  0.52382   1.2516    0.36535  -0.10563  -0.95343   0.3965\n",
      "  0.63945  -0.75734   1.0288    0.3788    0.59807  -0.4078   -0.5924\n",
      " -0.097769  0.88356  -0.49438  -0.819     1.5071    0.93786  -0.17713\n",
      "  0.20902 ]\n"
     ]
    }
   ],
   "source": [
    "word = \"specialization\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])\n",
    "print(word_to_vec_map[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "#from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers import Dense, Input, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sentences_to_indices\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        #sentence_words =X[i].lower().split()\n",
    "        \n",
    "        sentence_words=re.split(r\";| |!|,|\\?\",X[i].lower())\n",
    "\n",
    "\n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            if w=='':\n",
    "                continue\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            try:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                # Increment j to j + 1\n",
    "                j = j+1\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices\n",
    "\n",
    "#词在词向量里面的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['student' 'I hate it' 'this teacher is enthusiastic']\n",
      "X1_indices = [[345140.      0.      0.      0.      0.]\n",
      " [185457. 174339. 193716.      0.      0.]\n",
      " [358160. 354429. 192973. 137919.      0.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"student\", \"I hate it\", \"this teacher is enthusiastic\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pretrained_embedding_layer\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = 0.78858\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "#print(type(embedding_layer))\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][49])\n",
    "#这个应该是指的把词向量变成了一个矩阵，比如5000*50 的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Emojify_V2\n",
    "\n",
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(128,return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.4)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.4)(X)\n",
    "    \n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 52)                0         \n",
      "_________________________________________________________________\n",
      "embedding_14 (Embedding)     (None, 52, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 52, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 52, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,669\n",
      "Trainable params: 223,619\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 3)\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "477/477 [==============================] - 7s - loss: 0.0223 - acc: 0.9958     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3403f2e8>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 1, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s     \n",
      "\n",
      "Test accuracy =  0.756521739648736\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "#得到了测试集的每句话每个词的位置\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 3)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:NEGATIVE prediction: really enjoyed the writing assignments as a creative project but I do not feel I was able to fully grasp the concepts despite getting good marksPOSITIVE\n",
      "Expected emoji:NEGATIVE prediction: The feedback we receive from peers is usually limited to a couple of words and does not really help you assess how well you did\tSUGGESTION\n",
      "Expected emoji:NEGATIVE prediction: Not useful in my opinionPOSITIVE\n",
      "Expected emoji:NEGATIVE prediction: I got a bit impatient with that towards the end\t1POSITIVE\n",
      "Expected emoji:NEGATIVE prediction: I wish it was more into a solo topic It would be great if it was given only git/github lectures so the students can be more comfortable when it comes to uploading fles in the following coursesSUGGESTION\n",
      "Expected emoji:NEGATIVE prediction: Really easy coursePOSITIVE\n",
      "Expected emoji:NEGATIVE prediction: Very basic stuff - if you've ever done any coding before and have even a rudimentary knowledge of Github - you can skip this courseSUGGESTION\n",
      "Expected emoji:NEGATIVE prediction: Very superficial knowledge More pratical exercises are needed and daily problem examples The use of github and his pratical purpose is not clearSUGGESTION\n",
      "Expected emoji:NEGATIVE prediction: Nothing much importantPOSITIVE\n",
      "Expected emoji:NEGATIVE prediction: Not very usefulPOSITIVE\n",
      "Expected emoji:NEGATIVE prediction: some information was interesting but in general I do not think it was crucial for learning RPOSITIVE\n",
      "Expected emoji:NEGATIVE prediction: I believe that in some of the assingments the student is asked to do a few things that werent taught in the courseSUGGESTION\n",
      "Expected emoji:NEGATIVE prediction: And also during one of the quizzes the platform wasn't working propperly and I had to answere it many timesSUGGESTION\n",
      "Expected emoji:NEGATIVE prediction: This was inconvenient for them because I had to put in several support requests as I progressed through the course even though I installed as much as I was allowed toSUGGESTION\n",
      "Expected emoji:NEGATIVE prediction: I cannot believe there is a style of teaching where you never get to see the best way to do somethingPOSITIVE\n",
      "Expected emoji:POSITIVE prediction: as this helped me practically pin down concepts which I found difficult to grasp in the past\t2NEGATIVE\n",
      "Expected emoji:POSITIVE prediction: Excellente course for a basic insightNEGATIVE\n",
      "Expected emoji:POSITIVE prediction: I have never seen a more passionate teacher than thisNEGATIVE\n",
      "Expected emoji:POSITIVE prediction: Definitely made me realize that there is so much to learn and experience in the world of rigorous qualitative research that one cannot simply pick up when working in research-related roles in the industrySUGGESTION\n",
      "Expected emoji:POSITIVE prediction: But I do not think mooc is a suitable format for this courseNEGATIVE\n",
      "Expected emoji:POSITIVE prediction: difficult material was made easyNEGATIVE\n",
      "Expected emoji:POSITIVE prediction: I also liked that I could print the script for each modulethat made it a lot easier to reviseSUGGESTION\n",
      "Expected emoji:POSITIVE prediction: It helped me a lot to organize the concepts that I already had about research methodsI stayed motivated all the course periodSUGGESTION\n",
      "Expected emoji:POSITIVE prediction: I learned a lotNEGATIVE\n",
      "Expected emoji:POSITIVE prediction: the additional interviews after each week extended the material from the virtual classroom to the real world of scientific researchNEGATIVE\n",
      "Expected emoji:SUGGESTION prediction: Some assessments were also not clearas one interpretation of the assessment would not always lead to scoring maximum pointsPOSITIVE\n",
      "Expected emoji:SUGGESTION prediction: More on Analysis and Writing would be appreciatedPOSITIVE\n",
      "Expected emoji:SUGGESTION prediction: One thing only would make it perfect Instead of having the slides as notes  the same info could be organised differently in a word document as it would be easier to read and to print outNEGATIVE\n",
      "Expected emoji:SUGGESTION prediction: I think it would be really helpful if similar videos could exist for each of the written assignmentsPOSITIVE\n",
      "Expected emoji:SUGGESTION prediction: Having someone who teaches the course provide sample answers or discussion videos for each written assignment would add a lot of value and aid students' learningNEGATIVE\n",
      "Expected emoji:SUGGESTION prediction: a bit more detail in the examples would help rather than brief general descriptionsNEGATIVE\n",
      "Expected emoji:SUGGESTION prediction: My only quibble is that its geared toward the experimental sciences and so the exams and assignments emphasized concepts and research design issues that are less germane to other fieldsPOSITIVE\n",
      "Expected emoji:SUGGESTION prediction: Please spread out the course content on Git over 3 or 4 coursesNEGATIVE\n",
      "Expected emoji:SUGGESTION prediction: More direct examples would be nice Kinda like what they do on Data School Otherwise very well doneNEGATIVE\n",
      "Expected emoji:SUGGESTION prediction: Some of the instruction is not exactly clear Perhaps you all are attempting to make the student engage in some heuristic thinkingNEGATIVE\n",
      "Expected emoji:SUGGESTION prediction: I noticed that w/in the course video's there were numerous cases of misspelled words and even some incorrect information When it comes to a class like this  precise and clear instructions are a necessityNEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "C = 3\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ category(Y_test[i]) + ' prediction: '+ X_test[i] + category(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is not good\n",
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
    "x_test = np.array(['it is not good'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "#print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n",
    "print(x_test[0])\n",
    "print(category(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
